Project Wiki Crawl

you can find the project in: https://github.com/aharonlevy/wiki-crawler
By Aharon Levy

with Project Wiki Crawl you can for now crawl wikipedia and store the links from a given value in a data base. 

in the future the project will make a visual map of the pages connections ranking pages using 'pageRank'

You are free to copy, modify, and distribute <PROJECT NAME> with attribution under the terms of CC BY-SA 4.0
https://creativecommons.org/licenses/by-sa/4.0/

for Project Wiki Crawl to run smoothly you should install:
    - python 3
    - sqlite


how to run:
in the project folder on the console print:
"python wiki_crawl.py" and follow what the console asks for

the project is just initialized so it's not running
I'll add the documentation location on a later date

interested? want to help-out?
message me at linkedin: https://www.linkedin.com/in/aharon-levy/
or at my e-mail: mr.aharon.levy@gmail.com

thanks!
